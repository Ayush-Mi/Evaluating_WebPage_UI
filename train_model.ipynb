{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notboook to fintune pretrained SSD model to detect webpage elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import pickle\n",
    "from six import BytesIO\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amishra162/Documents/Coursera/work/lib/python3.9/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/Users/amishra162/Documents/Coursera/work/lib/python3.9/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: [\"[Errno 2] The file to load file system plugin from does not exist.: '/Users/amishra162/Documents/Coursera/work/lib/python3.9/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so'\"]\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/Users/amishra162/Documents/Coursera/work/lib/python3.9/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/Users/amishra162/Documents/Coursera/work/lib/python3.9/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: [\"dlopen(/Users/amishra162/Documents/Coursera/work/lib/python3.9/site-packages/tensorflow_io/python/ops/libtensorflow_io.so, 0x0006): tried: '/Users/amishra162/Documents/Coursera/work/lib/python3.9/site-packages/tensorflow_io/python/ops/libtensorflow_io.so' (no such file)\"]\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.builders import model_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load image and convert into numpy array\n",
    "\n",
    "def load_image_into_numpy_array(path):\n",
    "    \n",
    "    img_data = tf.io.gfile.GFile(path, 'rb').read()\n",
    "    image = (Image.open(BytesIO(img_data)))\n",
    "    (im_width, im_height) = image.size\n",
    "    \n",
    "    return np.array(image.getdata()).reshape(\n",
    "        (im_height, im_width, 3)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (76820, 8)\n",
      "Val data shape:  (20954, 8)\n",
      "Test data shape:  (10656, 8)\n"
     ]
    }
   ],
   "source": [
    "og_train_df = pd.read_csv('./data/train/_annotations.csv')\n",
    "\n",
    "# Arranging the bounding box coordinates as per SSD model \n",
    "# ymin xmin ymax xmax\n",
    "train_df = og_train_df.iloc[:,[0,1,2,3,5,4,7,6]]\n",
    "val_df = pd.read_csv('./data/valid/_annotations.csv')\n",
    "test_df = pd.read_csv('./data/test/_annotations.csv')\n",
    "\n",
    "print(\"Train data shape: \",train_df.shape)\n",
    "print(\"Val data shape: \",val_df.shape)\n",
    "print(\"Test data shape: \",test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the bounding box cordinates\n",
    "\n",
    "train_df['ymin'] = train_df['ymin']/train_df.ymax.max()\n",
    "\n",
    "train_df['xmin'] = train_df['xmin']/train_df.xmax.max()\n",
    "\n",
    "train_df['ymax'] = train_df['ymax']/train_df.ymax.max()\n",
    "\n",
    "train_df['xmax'] = train_df['xmax']/train_df.xmax.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>class</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymax</th>\n",
       "      <th>xmax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lavanguardia_com_png.rf.1070b85a3d6b62256b2ff8...</td>\n",
       "      <td>1024</td>\n",
       "      <td>768</td>\n",
       "      <td>text</td>\n",
       "      <td>0.763021</td>\n",
       "      <td>0.022461</td>\n",
       "      <td>0.955729</td>\n",
       "      <td>0.665039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lavanguardia_com_png.rf.1070b85a3d6b62256b2ff8...</td>\n",
       "      <td>1024</td>\n",
       "      <td>768</td>\n",
       "      <td>link</td>\n",
       "      <td>0.936198</td>\n",
       "      <td>0.443359</td>\n",
       "      <td>0.955729</td>\n",
       "      <td>0.567383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lavanguardia_com_png.rf.1070b85a3d6b62256b2ff8...</td>\n",
       "      <td>1024</td>\n",
       "      <td>768</td>\n",
       "      <td>text</td>\n",
       "      <td>0.936198</td>\n",
       "      <td>0.022461</td>\n",
       "      <td>0.980469</td>\n",
       "      <td>0.642578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lavanguardia_com_png.rf.1070b85a3d6b62256b2ff8...</td>\n",
       "      <td>1024</td>\n",
       "      <td>768</td>\n",
       "      <td>text</td>\n",
       "      <td>0.861979</td>\n",
       "      <td>0.715820</td>\n",
       "      <td>0.880208</td>\n",
       "      <td>0.789062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lavanguardia_com_png.rf.1070b85a3d6b62256b2ff8...</td>\n",
       "      <td>1024</td>\n",
       "      <td>768</td>\n",
       "      <td>button</td>\n",
       "      <td>0.846354</td>\n",
       "      <td>0.683594</td>\n",
       "      <td>0.897135</td>\n",
       "      <td>0.821289</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename  width  height   class  \\\n",
       "0  lavanguardia_com_png.rf.1070b85a3d6b62256b2ff8...   1024     768    text   \n",
       "1  lavanguardia_com_png.rf.1070b85a3d6b62256b2ff8...   1024     768    link   \n",
       "2  lavanguardia_com_png.rf.1070b85a3d6b62256b2ff8...   1024     768    text   \n",
       "3  lavanguardia_com_png.rf.1070b85a3d6b62256b2ff8...   1024     768    text   \n",
       "4  lavanguardia_com_png.rf.1070b85a3d6b62256b2ff8...   1024     768  button   \n",
       "\n",
       "       ymin      xmin      ymax      xmax  \n",
       "0  0.763021  0.022461  0.955729  0.665039  \n",
       "1  0.936198  0.443359  0.955729  0.567383  \n",
       "2  0.936198  0.022461  0.980469  0.642578  \n",
       "3  0.861979  0.715820  0.880208  0.789062  \n",
       "4  0.846354  0.683594  0.897135  0.821289  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "button     23872\n",
       "field       1558\n",
       "heading     6470\n",
       "iframe       608\n",
       "image      12486\n",
       "label        180\n",
       "link        9246\n",
       "text       22400\n",
       "Name: filename, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Class distribution\n",
    "classes = train_df['class'].unique()\n",
    "num_classes = len(classes)\n",
    "train_df.groupby(by='class').count()['filename']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train images:  (1688,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of train images: \",train_df['filename'].unique().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading all the images as numpy array\n",
    "img_path = train_df.filename.unique()\n",
    "train_images_np = [load_image_into_numpy_array('./data/train/'+path) for path in img_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hx/8ktl0wt56q7dx073nt1x6dsm0000gp/T/ipykernel_50553/1658986981.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  gt_boxes = np.array([train_df[train_df['filename']==i].drop(columns=['filename','width','height','class']).to_numpy() for i in img_path])\n",
      "/var/folders/hx/8ktl0wt56q7dx073nt1x6dsm0000gp/T/ipykernel_50553/1658986981.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  gt_classes = np.array([train_df[train_df['filename']==i]['class'] for i in img_path])\n"
     ]
    }
   ],
   "source": [
    "# Getting bounding box coordinates for each images and then converting them to tensors\n",
    "# Label Encoding the classes for each bounding box and then creating a category index dictionary for each class\n",
    "# classes are later one hot encoded to tensors\n",
    "\n",
    "gt_boxes = np.array([train_df[train_df['filename']==i].drop(columns=['filename','width','height','class']).to_numpy() for i in img_path])\n",
    "gt_classes = np.array([train_df[train_df['filename']==i]['class'] for i in img_path])\n",
    "\n",
    "class_label_list = []\n",
    "for i in range(gt_classes.shape[0]):\n",
    "    class_label_list.append(gt_classes[i].to_list())\n",
    "\n",
    "LE = LabelEncoder()\n",
    "label_encoder = LE.fit(classes)\n",
    "encoded_labels= [label_encoder.transform(gt_classes[i]) for i in range(gt_classes.shape[0])]\n",
    "\n",
    "category_index = {}\n",
    "for x in range(len(label_encoder.classes_)):\n",
    "    category_index[x+1] = {'id':x+1,'name':label_encoder.classes_[x]}\n",
    "\n",
    "gt_classes_one_hot_tensors = [tf.one_hot(encoded_labels[i], num_classes) for i in range(len(encoded_labels))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the image numpy array into tensors\n",
    "# converting ground truth boxes to tensors\n",
    "train_image_tensors = []\n",
    "\n",
    "gt_box_tensors = []\n",
    "\n",
    "for (train_image_np, gt_box_np) in zip(train_images_np, gt_boxes):\n",
    "    \n",
    "    train_image_tensors.append(tf.expand_dims(tf.convert_to_tensor(\n",
    "        train_image_np, dtype=tf.float32), axis=0))\n",
    "    \n",
    "    gt_box_tensors.append(tf.convert_to_tensor(gt_box_np, dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "pipeline_config = './ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/pipeline.config'\n",
    "\n",
    "# loading piepline config\n",
    "configs = config_util.get_configs_from_pipeline_file(pipeline_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modifying the last layer to number of classes in our problem i.e 10\n",
    "model_config = configs.get('model')\n",
    "model_config.ssd.num_classes = num_classes\n",
    "model_config.ssd.freeze_batchnorm = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building model with our modified configuration\n",
    "detection_model = model_builder.build(model_config=model_config, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create a temprory checkpoint to load only the required layer checkpoint\n",
    "\n",
    "tmp_box_predictor_checkpoint = tf.train.Checkpoint(\n",
    "    _base_tower_layers_for_heads = detection_model._box_predictor._base_tower_layers_for_heads,\n",
    "    _box_prediction_head = detection_model._box_predictor._box_prediction_head\n",
    ")  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_model_checkpoint = tf.train.Checkpoint(\n",
    "    _box_predictor = tmp_box_predictor_checkpoint,\n",
    "    _feature_extractor = detection_model._feature_extractor\n",
    ")          \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x34d316d30>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = './ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/checkpoint/ckpt-0'\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(\n",
    "    model=tmp_model_checkpoint\n",
    ")\n",
    "checkpoint.restore(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the detection model's `preprocess()` method and pass a dummy image\n",
    "tmp_image, tmp_shapes = detection_model.preprocess(tf.zeros([1, 640, 640, 3]))\n",
    "\n",
    "# run a prediction with the preprocessed image and shapes\n",
    "tmp_prediction_dict = detection_model.predict(tmp_image, tmp_shapes)\n",
    "\n",
    "# postprocess the predictions into final detections\n",
    "tmp_detections = detection_model.postprocess(tmp_prediction_dict, tmp_shapes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "269"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(detection_model.trainable_variables) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amishra162/Documents/Coursera/work/lib/python3.9/site-packages/keras/backend.py:450: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.set_learning_phase(True)\n",
    "\n",
    "batch_size = 5\n",
    "num_batches = int(len(img_path)/batch_size)\n",
    "learning_rate = 0.0001\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be fintuning all the layers in our network since our data is different from the coco dataset\n",
    "to_fine_tune = []\n",
    "for v in detection_model.trainable_variables:\n",
    "    to_fine_tune.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step_fn(image_list,\n",
    "                groundtruth_boxes_list,\n",
    "                groundtruth_classes_list,\n",
    "                model,\n",
    "                optimizer,\n",
    "                vars_to_fine_tune):\n",
    "\n",
    "    model.provide_groundtruth(\n",
    "        groundtruth_boxes_list=groundtruth_boxes_list,\n",
    "        groundtruth_classes_list=groundtruth_classes_list\n",
    "    )\n",
    "\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        preprocessed_image_list = []\n",
    "        true_shape_list = []\n",
    "\n",
    "        for img in image_list:\n",
    "            processed_img, true_shape = model.preprocess(img)\n",
    "            preprocessed_image_list.append(processed_img)\n",
    "            true_shape_list.append(true_shape)\n",
    "\n",
    "        preprocessed_image_tensor = tf.concat(preprocessed_image_list, axis=0)\n",
    "        true_shape_tensor = tf.concat(true_shape_list, axis=0)\n",
    "\n",
    "        prediction_dict = model.predict(preprocessed_image_tensor, true_shape_tensor)\n",
    "\n",
    "        losses_dict = model.loss(prediction_dict, true_shape_tensor)\n",
    "        \n",
    "        total_loss = losses_dict['Loss/localization_loss'] + losses_dict['Loss/classification_loss']\n",
    "\n",
    "        gradients = tape.gradient([total_loss], vars_to_fine_tune)\n",
    "\n",
    "        optimizer.apply_gradients(zip(gradients, vars_to_fine_tune))\n",
    "                \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epcoh 0batch 0 of 337, loss=1.7261318\n",
      "Epcoh 0batch 10 of 337, loss=1.5729825\n",
      "Epcoh 0batch 20 of 337, loss=1.4748225\n",
      "Epcoh 0batch 30 of 337, loss=1.3079951\n",
      "Epcoh 0batch 40 of 337, loss=1.2571833\n",
      "Epcoh 0batch 50 of 337, loss=1.1960187\n",
      "Epcoh 0batch 60 of 337, loss=1.345959\n",
      "Epcoh 0batch 70 of 337, loss=1.1256416\n",
      "Epcoh 0batch 80 of 337, loss=1.1253576\n",
      "Epcoh 0batch 90 of 337, loss=0.94458616\n",
      "Epcoh 0batch 100 of 337, loss=1.1281161\n",
      "Epcoh 0batch 110 of 337, loss=0.91451395\n",
      "Epcoh 0batch 120 of 337, loss=0.83039725\n",
      "Epcoh 0batch 130 of 337, loss=1.1309582\n",
      "Epcoh 0batch 140 of 337, loss=0.8774462\n",
      "Epcoh 0batch 150 of 337, loss=0.97723883\n",
      "Epcoh 0batch 160 of 337, loss=0.865576\n",
      "Epcoh 0batch 170 of 337, loss=0.9814497\n",
      "Epcoh 0batch 180 of 337, loss=0.9388566\n",
      "Epcoh 0batch 190 of 337, loss=0.8057493\n",
      "Epcoh 0batch 200 of 337, loss=1.1015687\n",
      "Epcoh 0batch 210 of 337, loss=0.97636473\n",
      "Epcoh 0batch 220 of 337, loss=0.76434267\n",
      "Epcoh 0batch 230 of 337, loss=0.85308707\n",
      "Epcoh 0batch 240 of 337, loss=0.9904704\n",
      "Epcoh 0batch 250 of 337, loss=0.88170046\n",
      "Epcoh 0batch 260 of 337, loss=0.7808858\n",
      "Epcoh 0batch 270 of 337, loss=0.71491027\n",
      "Epcoh 0batch 280 of 337, loss=0.80187285\n",
      "Epcoh 0batch 290 of 337, loss=0.9141384\n",
      "Epcoh 0batch 300 of 337, loss=0.82804394\n",
      "Epcoh 0batch 310 of 337, loss=1.0508784\n",
      "Epcoh 0batch 320 of 337, loss=0.9596596\n",
      "Epcoh 0batch 330 of 337, loss=0.9658567\n",
      "Epcoh 1batch 0 of 337, loss=1.0717185\n",
      "Epcoh 1batch 10 of 337, loss=0.8596385\n",
      "Epcoh 1batch 20 of 337, loss=0.70808566\n",
      "Epcoh 1batch 30 of 337, loss=0.86684906\n",
      "Epcoh 1batch 40 of 337, loss=0.82901824\n",
      "Epcoh 1batch 50 of 337, loss=0.9283779\n",
      "Epcoh 1batch 60 of 337, loss=0.65946937\n",
      "Epcoh 1batch 70 of 337, loss=0.8951986\n",
      "Epcoh 1batch 80 of 337, loss=0.89170414\n",
      "Epcoh 1batch 90 of 337, loss=0.68909466\n",
      "Epcoh 1batch 100 of 337, loss=1.2841479\n",
      "Epcoh 1batch 110 of 337, loss=0.7796803\n",
      "Epcoh 1batch 120 of 337, loss=0.85198236\n",
      "Epcoh 1batch 130 of 337, loss=0.68367195\n",
      "Epcoh 1batch 140 of 337, loss=0.6859397\n",
      "Epcoh 1batch 150 of 337, loss=0.7729993\n",
      "Epcoh 1batch 160 of 337, loss=0.6987001\n",
      "Epcoh 1batch 170 of 337, loss=0.9917142\n",
      "Epcoh 1batch 180 of 337, loss=1.0993353\n",
      "Epcoh 1batch 190 of 337, loss=0.6657275\n",
      "Epcoh 1batch 200 of 337, loss=0.63974184\n",
      "Epcoh 1batch 210 of 337, loss=0.8539251\n",
      "Epcoh 1batch 220 of 337, loss=0.7814329\n",
      "Epcoh 1batch 230 of 337, loss=0.7554239\n",
      "Epcoh 1batch 240 of 337, loss=0.93652844\n",
      "Epcoh 1batch 250 of 337, loss=0.86391723\n",
      "Epcoh 1batch 260 of 337, loss=0.5941563\n",
      "Epcoh 1batch 270 of 337, loss=0.8058086\n",
      "Epcoh 1batch 280 of 337, loss=0.86813766\n",
      "Epcoh 1batch 290 of 337, loss=0.6668465\n",
      "Epcoh 1batch 300 of 337, loss=1.126556\n",
      "Epcoh 1batch 310 of 337, loss=0.9122373\n",
      "Epcoh 1batch 320 of 337, loss=1.2279392\n",
      "Epcoh 1batch 330 of 337, loss=0.8662697\n",
      "Epcoh 2batch 0 of 337, loss=0.86695623\n",
      "Epcoh 2batch 10 of 337, loss=0.82280594\n",
      "Epcoh 2batch 20 of 337, loss=0.83814335\n",
      "Epcoh 2batch 30 of 337, loss=0.82627916\n",
      "Epcoh 2batch 40 of 337, loss=0.7794728\n",
      "Epcoh 2batch 50 of 337, loss=0.86577237\n",
      "Epcoh 2batch 60 of 337, loss=0.6754104\n",
      "Epcoh 2batch 70 of 337, loss=0.95395577\n",
      "Epcoh 2batch 80 of 337, loss=0.9410678\n",
      "Epcoh 2batch 90 of 337, loss=0.8646938\n",
      "Epcoh 2batch 100 of 337, loss=0.8097239\n",
      "Epcoh 2batch 110 of 337, loss=0.8670072\n",
      "Epcoh 2batch 120 of 337, loss=0.68284476\n",
      "Epcoh 2batch 130 of 337, loss=0.77136683\n",
      "Epcoh 2batch 140 of 337, loss=0.66535604\n",
      "Epcoh 2batch 150 of 337, loss=0.96532416\n",
      "Epcoh 2batch 160 of 337, loss=0.8885529\n",
      "Epcoh 2batch 170 of 337, loss=0.8481778\n",
      "Epcoh 2batch 180 of 337, loss=0.6212332\n",
      "Epcoh 2batch 190 of 337, loss=0.7154342\n",
      "Epcoh 2batch 200 of 337, loss=0.6428628\n",
      "Epcoh 2batch 210 of 337, loss=0.98846185\n",
      "Epcoh 2batch 220 of 337, loss=0.71723247\n",
      "Epcoh 2batch 230 of 337, loss=0.7198492\n",
      "Epcoh 2batch 240 of 337, loss=0.6739081\n",
      "Epcoh 2batch 250 of 337, loss=0.9114788\n",
      "Epcoh 2batch 260 of 337, loss=0.8860574\n",
      "Epcoh 2batch 270 of 337, loss=0.6526009\n",
      "Epcoh 2batch 280 of 337, loss=0.7517905\n",
      "Epcoh 2batch 290 of 337, loss=0.91390115\n",
      "Epcoh 2batch 300 of 337, loss=0.827539\n",
      "Epcoh 2batch 310 of 337, loss=0.64108616\n",
      "Epcoh 2batch 320 of 337, loss=0.6375417\n",
      "Epcoh 2batch 330 of 337, loss=0.8536463\n",
      "Epcoh 3batch 0 of 337, loss=1.0980668\n",
      "Epcoh 3batch 10 of 337, loss=0.90206695\n",
      "Epcoh 3batch 20 of 337, loss=0.8318276\n",
      "Epcoh 3batch 30 of 337, loss=0.5775202\n",
      "Epcoh 3batch 40 of 337, loss=0.8686055\n",
      "Epcoh 3batch 50 of 337, loss=0.66424334\n",
      "Epcoh 3batch 60 of 337, loss=0.56885505\n",
      "Epcoh 3batch 70 of 337, loss=0.86036456\n",
      "Epcoh 3batch 80 of 337, loss=0.6954032\n",
      "Epcoh 3batch 90 of 337, loss=0.51412916\n",
      "Epcoh 3batch 100 of 337, loss=0.7914016\n",
      "Epcoh 3batch 110 of 337, loss=0.47224885\n",
      "Epcoh 3batch 120 of 337, loss=0.60606635\n",
      "Epcoh 3batch 130 of 337, loss=0.68252754\n",
      "Epcoh 3batch 140 of 337, loss=0.8087853\n",
      "Epcoh 3batch 150 of 337, loss=1.012357\n",
      "Epcoh 3batch 160 of 337, loss=0.6572963\n",
      "Epcoh 3batch 170 of 337, loss=0.71038496\n",
      "Epcoh 3batch 180 of 337, loss=0.6681107\n",
      "Epcoh 3batch 190 of 337, loss=0.7559277\n",
      "Epcoh 3batch 200 of 337, loss=0.5798062\n",
      "Epcoh 3batch 210 of 337, loss=0.6312945\n",
      "Epcoh 3batch 220 of 337, loss=0.6995288\n",
      "Epcoh 3batch 230 of 337, loss=0.73669505\n",
      "Epcoh 3batch 240 of 337, loss=0.7139238\n",
      "Epcoh 3batch 250 of 337, loss=0.5838373\n",
      "Epcoh 3batch 260 of 337, loss=0.7196979\n",
      "Epcoh 3batch 270 of 337, loss=0.7187406\n",
      "Epcoh 3batch 280 of 337, loss=0.7044995\n",
      "Epcoh 3batch 290 of 337, loss=0.80529135\n",
      "Epcoh 3batch 300 of 337, loss=0.77993923\n",
      "Epcoh 3batch 310 of 337, loss=0.70151514\n",
      "Epcoh 3batch 320 of 337, loss=0.67755723\n",
      "Epcoh 3batch 330 of 337, loss=0.87999475\n",
      "Epcoh 4batch 0 of 337, loss=0.8435749\n",
      "Epcoh 4batch 10 of 337, loss=0.64941317\n",
      "Epcoh 4batch 20 of 337, loss=1.2482531\n",
      "Epcoh 4batch 30 of 337, loss=0.7502268\n",
      "Epcoh 4batch 40 of 337, loss=0.68371344\n",
      "Epcoh 4batch 50 of 337, loss=0.63648975\n",
      "Epcoh 4batch 60 of 337, loss=0.5787318\n",
      "Epcoh 4batch 70 of 337, loss=0.6554618\n",
      "Epcoh 4batch 80 of 337, loss=0.61462307\n",
      "Epcoh 4batch 90 of 337, loss=0.6377895\n",
      "Epcoh 4batch 100 of 337, loss=0.63944215\n",
      "Epcoh 4batch 110 of 337, loss=0.77895874\n",
      "Epcoh 4batch 120 of 337, loss=0.65944576\n",
      "Epcoh 4batch 130 of 337, loss=0.6195514\n",
      "Epcoh 4batch 140 of 337, loss=0.89791226\n",
      "Epcoh 4batch 150 of 337, loss=0.6806397\n",
      "Epcoh 4batch 160 of 337, loss=0.71890366\n",
      "Epcoh 4batch 170 of 337, loss=0.6904494\n",
      "Epcoh 4batch 180 of 337, loss=0.7834693\n",
      "Epcoh 4batch 190 of 337, loss=0.9168149\n",
      "Epcoh 4batch 200 of 337, loss=0.8078314\n",
      "Epcoh 4batch 210 of 337, loss=0.59770685\n",
      "Epcoh 4batch 220 of 337, loss=0.73268586\n",
      "Epcoh 4batch 230 of 337, loss=0.6433345\n",
      "Epcoh 4batch 240 of 337, loss=0.77493155\n",
      "Epcoh 4batch 250 of 337, loss=0.6509035\n",
      "Epcoh 4batch 260 of 337, loss=0.7350756\n",
      "Epcoh 4batch 270 of 337, loss=0.86416924\n",
      "Epcoh 4batch 280 of 337, loss=1.0659045\n",
      "Epcoh 4batch 290 of 337, loss=0.72884303\n",
      "Epcoh 4batch 300 of 337, loss=0.5256738\n",
      "Epcoh 4batch 310 of 337, loss=0.50221527\n",
      "Epcoh 4batch 320 of 337, loss=0.6706908\n",
      "Epcoh 4batch 330 of 337, loss=0.59954244\n"
     ]
    }
   ],
   "source": [
    "# Training model for 5 epochs\n",
    "\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "loss_monitor = {}\n",
    "for i in range(epochs):\n",
    "    for idx in range(num_batches):\n",
    "        all_keys = list(range(len(train_images_np)))\n",
    "        random.shuffle(all_keys)\n",
    "        example_keys = all_keys[:batch_size]\n",
    "\n",
    "        gt_boxes_list = [gt_box_tensors[key] for key in example_keys]\n",
    "        gt_classes_list = [gt_classes_one_hot_tensors[key] for key in example_keys]\n",
    "        \n",
    "        image_tensors = [train_image_tensors[key] for key in example_keys]\n",
    "\n",
    "        total_loss = train_step_fn(image_tensors, \n",
    "                                gt_boxes_list, \n",
    "                                gt_classes_list,\n",
    "                                detection_model,\n",
    "                                optimizer,\n",
    "                                to_fine_tune\n",
    "                                )\n",
    "\n",
    "        if idx % 10 == 0:\n",
    "            print('Epcoh ' + str(i) + 'batch ' + str(idx) + ' of ' + str(num_batches)\n",
    "            + ', loss=' +  str(total_loss.numpy()), flush=True)\n",
    "            loss_monitor[i] = {str(idx):str(total_loss.numpy())}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x34d92e940>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as WeightSharedConvolutionalBoxPredictor_layer_call_fn, WeightSharedConvolutionalBoxPredictor_layer_call_and_return_conditional_losses, WeightSharedConvolutionalBoxHead_layer_call_fn, WeightSharedConvolutionalBoxHead_layer_call_and_return_conditional_losses, WeightSharedConvolutionalClassHead_layer_call_fn while saving (showing 5 of 278). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: web_element_detection/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: web_element_detection/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(detection_model, 'web_element_detection', signatures=None, options=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing pipeline config file to ./web_element_detection/new_config/pipeline.config\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing pipeline config file to ./web_element_detection/new_config/pipeline.config\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./web_element_detection/checkpoint/ckpt-1'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save new pipeline config\n",
    "new_pipeline_proto = config_util.create_pipeline_proto_from_configs(configs)\n",
    "config_util.save_pipeline_config(new_pipeline_proto, './web_element_detection/new_config')\n",
    "\n",
    "exported_ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt_manager = tf.train.CheckpointManager(exported_ckpt, directory=\"./web_element_detection/checkpoint/\", max_to_keep=None)\n",
    "\n",
    "ckpt_manager.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./web_element_detection/classes.pkl', 'wb') as f:\n",
    "    pickle.dump(category_index, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('work': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cbb4c99ba6a158315ca45ade86b338397709de922c2faeb37289f32131d55cc1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
